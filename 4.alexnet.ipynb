{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/101_ObjectCategories/octopus', '/data/101_ObjectCategories/trilobite', '/data/101_ObjectCategories/chandelier', '/data/101_ObjectCategories/butterfly', '/data/101_ObjectCategories/hawksbill', '/data/101_ObjectCategories/laptop', '/data/101_ObjectCategories/yin_yang', '/data/101_ObjectCategories/pizza', '/data/101_ObjectCategories/umbrella', '/data/101_ObjectCategories/helicopter', '/data/101_ObjectCategories/ibis', '/data/101_ObjectCategories/dragonfly', '/data/101_ObjectCategories/cellphone', '/data/101_ObjectCategories/dollar_bill', '/data/101_ObjectCategories/scissors', '/data/101_ObjectCategories/tick', '/data/101_ObjectCategories/crab', '/data/101_ObjectCategories/cougar_body', '/data/101_ObjectCategories/cup', '/data/101_ObjectCategories/ant', '/data/101_ObjectCategories/BACKGROUND_Google', '/data/101_ObjectCategories/schooner', '/data/101_ObjectCategories/wild_cat', '/data/101_ObjectCategories/beaver', '/data/101_ObjectCategories/llama', '/data/101_ObjectCategories/grand_piano', '/data/101_ObjectCategories/menorah', '/data/101_ObjectCategories/Faces_easy', '/data/101_ObjectCategories/platypus', '/data/101_ObjectCategories/buddha', '/data/101_ObjectCategories/anchor', '/data/101_ObjectCategories/headphone', '/data/101_ObjectCategories/electric_guitar', '/data/101_ObjectCategories/rooster', '/data/101_ObjectCategories/starfish', '/data/101_ObjectCategories/pyramid', '/data/101_ObjectCategories/crocodile', '/data/101_ObjectCategories/stop_sign', '/data/101_ObjectCategories/airplanes', '/data/101_ObjectCategories/dolphin', '/data/101_ObjectCategories/metronome', '/data/101_ObjectCategories/gramophone', '/data/101_ObjectCategories/lobster', '/data/101_ObjectCategories/rhino', '/data/101_ObjectCategories/ferry', '/data/101_ObjectCategories/flamingo_head', '/data/101_ObjectCategories/bonsai', '/data/101_ObjectCategories/car_side', '/data/101_ObjectCategories/water_lilly', '/data/101_ObjectCategories/Motorbikes', '/data/101_ObjectCategories/wrench', '/data/101_ObjectCategories/elephant', '/data/101_ObjectCategories/soccer_ball', '/data/101_ObjectCategories/hedgehog', '/data/101_ObjectCategories/garfield', '/data/101_ObjectCategories/mayfly', '/data/101_ObjectCategories/chair', '/data/101_ObjectCategories/joshua_tree', '/data/101_ObjectCategories/brain', '/data/101_ObjectCategories/brontosaurus', '/data/101_ObjectCategories/bass', '/data/101_ObjectCategories/ceiling_fan', '/data/101_ObjectCategories/panda', '/data/101_ObjectCategories/flamingo', '/data/101_ObjectCategories/pagoda', '/data/101_ObjectCategories/ewer', '/data/101_ObjectCategories/okapi', '/data/101_ObjectCategories/sunflower', '/data/101_ObjectCategories/mandolin', '/data/101_ObjectCategories/cannon', '/data/101_ObjectCategories/accordion', '/data/101_ObjectCategories/gerenuk', '/data/101_ObjectCategories/wheelchair', '/data/101_ObjectCategories/pigeon', '/data/101_ObjectCategories/crayfish', '/data/101_ObjectCategories/scorpion', '/data/101_ObjectCategories/lotus', '/data/101_ObjectCategories/strawberry', '/data/101_ObjectCategories/Leopards', '/data/101_ObjectCategories/stapler', '/data/101_ObjectCategories/Faces', '/data/101_ObjectCategories/crocodile_head', '/data/101_ObjectCategories/minaret', '/data/101_ObjectCategories/inline_skate', '/data/101_ObjectCategories/watch', '/data/101_ObjectCategories/barrel', '/data/101_ObjectCategories/euphonium', '/data/101_ObjectCategories/sea_horse', '/data/101_ObjectCategories/revolver', '/data/101_ObjectCategories/dalmatian', '/data/101_ObjectCategories/camera', '/data/101_ObjectCategories/nautilus', '/data/101_ObjectCategories/cougar_face', '/data/101_ObjectCategories/binocular', '/data/101_ObjectCategories/snoopy', '/data/101_ObjectCategories/kangaroo', '/data/101_ObjectCategories/emu', '/data/101_ObjectCategories/ketch', '/data/101_ObjectCategories/stegosaurus', '/data/101_ObjectCategories/saxophone', '/data/101_ObjectCategories/lamp', '/data/101_ObjectCategories/windsor_chair']\n",
      "conv1/Relu   [None, 56, 56, 96]\n",
      "lrn1/pool1   [None, 27, 27, 96]\n",
      "conv2/Relu   [None, 27, 27, 256]\n",
      "lrn2/pool2   [None, 13, 13, 256]\n",
      "conv3/Relu   [None, 13, 13, 384]\n",
      "conv4/Relu   [None, 13, 13, 384]\n",
      "conv5/Relu   [None, 13, 13, 256]\n",
      "fc6/Tanh   [None, 1, 1, 4096]\n",
      "fc7/Tanh   [None, 1, 1, 4096]\n",
      "fc8/Reshape_1   [None, 1, 1, 2]\n",
      "Tensor(\"strided_slice:0\", shape=(2,), dtype=float32)\n",
      "[0 1]\n",
      "[0.95289725 0.0471027 ]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 0]\n",
      "Iter0.0,训练集损失量0.285331训练集正确率,0.58000\n",
      "[0 1]\n",
      "[0.00386458 0.9961355 ]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.311240训练集正确率,0.70000\n",
      "[0 1]\n",
      "[3.4882905e-09 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.324679训练集正确率,0.70000\n",
      "[0 1]\n",
      "[2.9892256e-08 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.276916训练集正确率,0.68000\n",
      "[0 1]\n",
      "[3.810912e-06 9.999962e-01]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.317072训练集正确率,0.68000\n",
      "[0 1]\n",
      "[2.6965306e-08 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.312399训练集正确率,0.70000\n",
      "[0 1]\n",
      "[3.2028797e-09 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.319918训练集正确率,0.70000\n",
      "[0 1]\n",
      "[9.483444e-12 1.000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.319912训练集正确率,0.68000\n",
      "[0 1]\n",
      "[1.3968125e-10 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.319307训练集正确率,0.68000\n",
      "[0 1]\n",
      "[5.5081394e-14 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.319981训练集正确率,0.68000\n",
      "[0 1]\n",
      "[1.0363666e-07 9.9999988e-01]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.319918训练集正确率,0.68000\n",
      "[0 1]\n",
      "[2.7499782e-11 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.312728训练集正确率,0.68000\n",
      "[0 1]\n",
      "[5.9784737e-09 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[6.847613e-16 1.000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[1.2288148e-09 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[1.739513e-13 1.000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.319991训练集正确率,0.68000\n",
      "[0 1]\n",
      "[2.9565566e-17 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[1.5744981e-17 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[7.263439e-14 1.000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[4.0408914e-15 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[4.907549e-15 1.000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[1.1585023e-17 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[3.4516872e-11 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[3.879727e-18 1.000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[1.0356027e-17 1.0000000e+00]\n",
      "[1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,训练集损失量0.320000训练集正确率,0.68000\n",
      "[0 1]\n",
      "[5.1543575e-16 1.0000000e+00]\n",
      "[1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iter0.0,测试集损失量0.354839测试集正确率,0.64516\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FLAGS=None\n",
    "scope=\"relu\"\n",
    "classes = 2\n",
    "def print_activation(t):\n",
    "    print(t.op.name,' ',t.get_shape().as_list())\n",
    "\n",
    "def inference(weights,biases_add):\n",
    "    images=tf.placeholder(\"float\",shape=[None,224,224,3],name='x')\n",
    "\n",
    "    parameters=[]\n",
    "\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        kernel = weights['wc1']\n",
    "        conv=tf.nn.conv2d(images,kernel,[1,4,4,1],padding='SAME')\n",
    "        biases=biases_add['wc1']\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        convl=tf.nn.relu(bias)\n",
    "        print_activation(convl)\n",
    "        parameters+=[kernel,biases]\n",
    "\n",
    "    with tf.name_scope('lrn1') as scope:\n",
    "        lrn1=tf.nn.local_response_normalization(convl,alpha=1e-4,beta=0.75,depth_radius=2,bias=2.0)\n",
    "\n",
    "        pool1=tf.nn.max_pool(lrn1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID',name='pool1')\n",
    "        print_activation(pool1)\n",
    "\n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        kernel=weights['wc2']\n",
    "        conv = tf.nn.conv2d(pool1,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=biases_add['wc2']\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv2=tf.nn.relu(bias)\n",
    "        parameters+=[kernel,biases]\n",
    "        print_activation(conv2)\n",
    "\n",
    "    with tf.name_scope('lrn2') as scope:\n",
    "        lrn2=tf.nn.local_response_normalization(conv2,alpha=1e-4,beta=0.75,depth_radius=2,bias=2.0)\n",
    "        pool2=tf.nn.max_pool(lrn2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID',name='pool2')\n",
    "        print_activation(pool2)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        kernel=weights['wc3']\n",
    "        conv=tf.nn.conv2d(pool2,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=biases_add['wc3']\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv3 = tf.nn.relu(bias)\n",
    "        parameters+=[kernel,biases]\n",
    "        print_activation(conv3)\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        kernel=weights['wc4']\n",
    "        conv=tf.nn.conv2d(conv3,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=biases_add['wc4']\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv4 = tf.nn.relu(bias)\n",
    "        parameters+=[kernel,biases]\n",
    "        print_activation(conv4)\n",
    "\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        kernel=weights['wc5']\n",
    "        conv=tf.nn.conv2d(conv4,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=biases_add['wc5']\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv5 = tf.nn.relu(bias)\n",
    "        parameters+=[kernel,biases]\n",
    "        print_activation(conv5)\n",
    "        pool5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    with tf.name_scope('fc6') as scope:\n",
    "        kernel=weights['fc6']\n",
    "        conv=tf.nn.conv2d(pool5,kernel,[1,1,1,1],padding='VALID')\n",
    "        biases=biases_add['fc6']\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        fc6 = tf.nn.tanh(bias)\n",
    "        parameters+=[kernel,biases]\n",
    "        print_activation(fc6)\n",
    "\n",
    "        fc6=tf.nn.dropout(fc6,0.5)\n",
    "\n",
    "    with tf.name_scope('fc7') as scope:\n",
    "        kernel = weights['fc7']\n",
    "        conv = tf.nn.conv2d(fc6, kernel, [1, 1, 1, 1], padding='VALID')\n",
    "        biases = biases_add['fc7']\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        fc7 = tf.nn.tanh(bias)\n",
    "        parameters += [kernel, biases]\n",
    "        print_activation(fc7)\n",
    "\n",
    "        fc8 = tf.nn.dropout(fc7, 0.5)\n",
    "\n",
    "    with tf.name_scope('fc8') as scope:\n",
    "        kernel = weights['fc8']\n",
    "        conv = tf.nn.conv2d(fc7, kernel, [1, 1, 1, 1], padding='VALID')\n",
    "        biases = biases_add['fc8']\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        fc8 = tf.nn.softmax(bias)\n",
    "        parameters += [kernel, biases]\n",
    "        print_activation(fc8)\n",
    "        y_pred_cls=fc8[:,0,0,:]\n",
    "\n",
    "    print(y_pred_cls[0,:])\n",
    "    return images,y_pred_cls\n",
    "\n",
    "def img_resize(filename):\n",
    "    image_batch=[]\n",
    "    for j in range(len(filename)):\n",
    "        a=filename[j]\n",
    "        img=cv2.imread(filename[j])\n",
    "        row_num,column_num=img.shape[0:2]\n",
    "        num=row_num\n",
    "        if row_num<column_num:\n",
    "            num=column_num\n",
    "        img2=np.zeros([num,num,3])\n",
    "        img2[0:row_num,0:column_num,:]=img\n",
    "        img3=cv2.resize(img2,(224,224),interpolation=cv2.INTER_CUBIC)\n",
    "        image_batch.append(img3)\n",
    "\n",
    "    image_batch=np.array(image_batch)\n",
    "\n",
    "    return image_batch\n",
    "\n",
    "img_path=[]\n",
    "\n",
    "def loadpath(input_dir):\n",
    "    for(path,dirnames,filenames) in os.walk(input_dir): \n",
    "        \n",
    "        for dirname in dirnames:\n",
    "            img_path.append(path+'/'+dirname)\n",
    "        return img_path\n",
    "\n",
    "def images_get(train_data_dir):\n",
    "    path=loadpath(train_data_dir)\n",
    "    imgs=[]\n",
    "    labs=[]\n",
    "    print(path)\n",
    "    img_num=np.zeros(len(path))\n",
    "    def readData(paths):\n",
    "        i=1\n",
    "        for path in paths[0:classes]:\n",
    "            i=i+1\n",
    "            j=0\n",
    "\n",
    "            for filename in os.listdir(path):\n",
    "                if(filename.endswith('.jpg')and j<=40):\n",
    "                    filename= path+'/'+filename\n",
    "                    imgs.append(filename)\n",
    "                    labs.append(path)\n",
    "            img_num[i]=j\n",
    "    readData(path)\n",
    "    data_dummy=pd.get_dummies(labs)\n",
    "    labs=np.array(data_dummy)\n",
    "    return imgs,labs\n",
    "\n",
    "def run_benchmark():\n",
    "    train_data_dir = \"/data/101_ObjectCategories\"\n",
    "    imgs,labs=images_get(train_data_dir)\n",
    "    imgs,x_test,labs,y_test=train_test_split(imgs,labs,test_size=0.25,shuffle=True)\n",
    "    i=0\n",
    "    with tf.Graph().as_default():\n",
    "        image_size=224\n",
    "        i=i+1;\n",
    "\n",
    "        weights={\n",
    "            'wc1':tf.Variable(tf.truncated_normal([11,11,3,96],dtype=tf.float32,stddev=1e-1),name='weights'),\n",
    "            'wc2': tf.Variable(tf.truncated_normal([5, 5, 96, 256], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "            'wc3': tf.Variable(tf.truncated_normal([3, 3, 256, 384], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "            'wc4': tf.Variable(tf.truncated_normal([3, 3, 384, 384], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "            'wc5': tf.Variable(tf.truncated_normal([3, 3, 384, 256], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "            'fc6': tf.Variable(tf.truncated_normal([6, 6, 256, 4096], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "            'fc7': tf.Variable(tf.truncated_normal([1, 1, 4096, 4096], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "            'fc8': tf.Variable(tf.truncated_normal([1, 1, 4096, classes], dtype=tf.float32, stddev=1e-1), name='weights'),\n",
    "\n",
    "        }\n",
    "        biases={\n",
    "            'wc1': tf.Variable(tf.constant(0.0,shape=[96],dtype=tf.float32),trainable=True,name='biases'),\n",
    "            'wc2': tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases'),\n",
    "            'wc3': tf.Variable(tf.constant(0.0,shape=[384],dtype=tf.float32),trainable=True,name='biases'),\n",
    "            'wc4': tf.Variable(tf.constant(0.0,shape=[384],dtype=tf.float32),trainable=True,name='biases'),\n",
    "            'wc5': tf.Variable(tf.constant(0.0,shape=[256],dtype=tf.float32),trainable=True,name='biases'),\n",
    "            'fc6': tf.Variable(tf.constant(0.0,shape=[4096],dtype=tf.float32),trainable=True,name='biases'),\n",
    "            'fc7': tf.Variable(tf.constant(0.0,shape=[4096],dtype=tf.float32),trainable=True,name='biases'),\n",
    "            'fc8': tf.Variable(tf.constant(0.0,shape=[classes],dtype=tf.float32),trainable=True,name='biases'),\n",
    "        }\n",
    "        learning_rate=0.001\n",
    "        y=tf.placeholder(\"float\",shape=[None,classes],name='y_true')\n",
    "        x,y_pred=inference(weights,biases)\n",
    "        cost=tf.reduce_mean(tf.square(y_pred-y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "        init=tf.global_variables_initializer()\n",
    "        sess=tf.Session()\n",
    "        sess.run(init)\n",
    "        batch=50\n",
    "        display_step=5\n",
    "        itr_num=int(len(imgs)/batch)\n",
    "        step=10\n",
    "        dropout=0.75\n",
    "        bench_num=25\n",
    "        for bench in range(bench_num):\n",
    "            for itr in range(itr_num):\n",
    "                x_batch = img_resize(imgs[batch*itr:batch*(itr+1)])\n",
    "                y_batch=labs[batch*itr:batch*(itr+1),:]\n",
    "\n",
    "                sess.run(optimizer,feed_dict={x:x_batch,y:y_batch})\n",
    "\n",
    "                y1=sess.run(y_pred,feed_dict={x:x_batch,y:y_batch})\n",
    "                correct_pred=tf.equal(np.argmax(y1,1),np.argmax(y_batch,1))\n",
    "                accuracy=tf.reduce_mean(tf.cast(correct_pred,\"float\"))\n",
    "\n",
    "                print(y_batch[0])\n",
    "                print(y1[0])\n",
    "                print(np.argmax(y_batch,1))\n",
    "                print(np.argmax(y1,1))\n",
    "                if step%display_step==0:\n",
    "                    loss,acc=sess.run([cost,accuracy],feed_dict={x:x_batch,y:y_batch})\n",
    "                    print(\"Iter\"+str(itr/itr_num)+\",训练集损失量\"+\"{:.6f}\".format(loss)+\"训练集正确率,\"+\"{:.5f}\".format(acc))\n",
    "\n",
    "        x_batch = img_resize(x_test)\n",
    "        y_batch = y_test\n",
    "\n",
    "        y1=sess.run(y_pred,feed_dict={x:x_batch})\n",
    "        correct_pred=tf.equal(np.argmax(y1,1),np.argmax(y_batch,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "\n",
    "        print(y_batch[0])\n",
    "        print(y1[0])\n",
    "        print(np.argmax(y_batch, 1))\n",
    "        print(np.argmax(y1, 1))\n",
    "\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: x_batch, y: y_batch})\n",
    "        print(\"Iter\" + str(itr / itr_num) + \",测试集损失量\" + \"{:.6f}\".format(loss) + \"测试集正确率,\" + \"{:.5f}\".format(acc))\n",
    "\n",
    "\n",
    "run_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
